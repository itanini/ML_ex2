{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student #1 ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student #2 ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Those tests will not be graded nor checked.\n",
    "1. You are free to add code and markdown cells as you see fit.\n",
    "1. Write your functions in this jupyter notebook only. Do not create external python modules and import from them.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only, unless otherwise mentioned.\n",
    "1. Your code must run without errors. During the environment setup, you were given a specific version of Python of install (`Python >= 3.6, numpy >= 1.14`). \n",
    "1. Answers to qualitative questions should be written in **markdown cells (with $\\LaTeX$ support)**.\n",
    "1. Submit this jupyter notebook only using your ID as a filename. No not use ZIP or RAR. For example, your submission should look like this: `123456789.ipynb` if you worked by yourself or `123456789_987654321.ipynb` if you worked in pairs.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Load a dataset and perform basic data exploration using a powerful data science library called [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "1. Pre-process the data for linear regression.\n",
    "1. Compute the cost and perform gradient descent in pure numpy in vectorized form.\n",
    "1. Perform multivariate linear regression.\n",
    "1. Visualize your results using matplotlib.\n",
    "1. Preform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Data Preprocessing (5 Points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County, USA. The dataset contains 21,613 observations with 17 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('kc_house_data.csv') # Relative paths are sometimes better than absolute paths.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in any data-oriented project is to first try and understand the data. Fortunately, pandas is built for that purpose. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b9d27f913f08c27",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Print the first 10 entries of the dataframe. \n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Show the statistics of the dataset. \n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Since we are dealing with simple linear regression, we will extract the target values and the `sqft_living` variable from the dataset. Use pandas and select both columns as separate variables and transform them into a numpy array.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before performing linear regression, we notice that some of the features are clearly irrelevant. Remove the features \n",
    "`id` and `date`. from the dataframe and save the values of the relevant feature in a dedicated variable as a numpy array. Save the targets as a different variable, also as a numpy array.\n",
    "\n",
    "We need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant. The features your should ignore are the `id` and `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None # placeholder for the variables\n",
    "y = None # placeholder for the target values\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes.\n",
    "\n",
    "Implement the cost function `preprocess` and make sure you are using vectorized operations (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform min-max scaling for both the data and the targets.\n",
    "    Input:\n",
    "    - X: Inputs (n features, m instances).\n",
    "    - y: True labels (1 target, m instances).\n",
    "\n",
    "    Output:\n",
    "    - X: The scaled inputs.\n",
    "    - y: The scaled labels.\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement Min-Max Scaling.                                        #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n",
    "\n",
    "Many real-world datasets, such as the dataset we are dealing with, are highly dimensional and cannot be visualized naively. <br>\n",
    "However, we can choose a feature and visualize the target price as a function of that feature. Pay close attention to the range of the axis, and include axis labels and a title for the figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Choose one fearture an plot the target price as a function of that feature\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that the data variable `X` supports the bias term $\\theta_0$. \n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\vec{\\theta^T} \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "Add columns of ones as the zeroth column of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Multi Variable Linear Regression (65 Points)\n",
    "\n",
    "In this part we will create a multivariate linear model and the logic needed to trained it using the given data.\n",
    "\n",
    "Our task is to find a linear model that best explains our dataset. We start by guessing initial values for the linear regression parameters $\\vec{\\theta}$ and updating the values using gradient descent. The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{n}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f83af93c0436542",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the cost function `compute_cost`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "\n",
    "    Output:\n",
    "    - J: the cost associated with the current set of parameters (single number).\n",
    "    \"\"\"\n",
    "    \n",
    "    J = 0  # Use J for the cost.\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the MSE cost function.                                  #\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # seeding the random number generator allows us to obtain reproducible results\n",
    "theta = np.array(np.random.random(size=X.shape[1]))\n",
    "compute_cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afdc527b73d275bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the function `gradient_descent`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent. Gradient descent\n",
    "    is an optimization algorithm used to minimize a (loss) function by \n",
    "    iteratively moving in the direction of steepest descent as defined by the\n",
    "    opposite direction of the gradient. Instead of performing a constant number\n",
    "    of iterations, stop the training process once the loss improvement from\n",
    "    one iteration to the next is smaller than `1e-8`.\n",
    "    \n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of the model.\n",
    "    - num_iters: The number of iterations performed.\n",
    "\n",
    "    Output:\n",
    "    - theta: The learned parameters of the model.\n",
    "    - J_history: the loss value in each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=X.shape[1])\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X ,y, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. Visualize the loss as a function of the iterations using the `J_history` array. This might help you find problems with your code and might indicate that your model fails to converge. Your visualization should be clear and include a title, labels and a proper scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Linear regression can also be solved by using the pseudo-inverse method. Implement the following function **without using `np.pinv`**. Instead, use direct matrix multiplication as you saw in class. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "\n",
    "    Outpu:\n",
    "    - theta: The optimal parameters of your model.\n",
    "\n",
    "    ########## DO NOT USE numpy.pinv ##############\n",
    "    \"\"\"\n",
    "    pinv_theta = [] # Use a python list to save cost in every iteration\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the pseudoinverse algorithm.                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return pinv_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X,y)\n",
    "J_pinv = compute_cost(X, y, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the results of the previous section to assess the convergence of the gradient descent process. Explain and use clear visualziations.\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model in terms of speed and accuracy. Complete the function `find_best_alpha`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_alpha(X, y, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over the provided values of alpha and maintain a python \n",
    "    dictionary with alpha as the key and the final loss as the value.\n",
    "    For consistent results, use the same theta value for all runs.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - num_iters: The number of iterations performed.\n",
    "\n",
    "    Output:\n",
    "    - alpha_dict: A python dictionary containing alpha as the \n",
    "                  key and the final loss as the value\n",
    "    \"\"\"\n",
    "    \n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_dict = find_best_alpha(X, y, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`. This can be done in a single line using built-in functions. \n",
    "\n",
    "**Explain the differences between the performance characteristics you observe.**\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = None\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Pick the best three alpha values you just calculated and provide **one** graph with three lines indicating the loss as a function of iterations. Note you are required to provide general code for this purpose (no hard-coding). Make sure the visualization is clear and informative. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Time for yet another visual sanity check. Create two scatter plots on the same figure: on one side, plot the predictions you obtained from a model trained using the alpha you previously found vs the predictions calculated using the optimal theta calculated using the pseudo-inverse. On the other size, create a scatter plot showing your model predictions vs the target values.\n",
    "\n",
    "What do you expect to see? Explain the results.\n",
    " \n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 3: feature selection (30 points)\n",
    "\n",
    "Adding additional features to our regression model makes it more complicated but does not necessarily improves performance. Find the combination of three features that best minimizes the loss on the validation set. First, we will reload the dataset as a dataframe in order to access the feature names. Use the dataframe with the relevant features as the input to the `generate_triplets` and obtain a list of all possible feature triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def generate_triplets(X):\n",
    "    \"\"\"\n",
    "    generate all possible sets of three features out of all relevant features\n",
    "    available from the given dataset X. Hint: check out the python package\n",
    "    'itertools'.di\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "\n",
    "    Output:\n",
    "    - A python list containing all feature triplets as integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    triplets = []\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0800ea03f680996e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "triplets = generate_triplets(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bf66bfd62450001",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to choose the best triplet possible, we will train a model using the training dataset (70%) and evaluate its performance on the validation dataset (20%). It is crucial to randomly split the dataset to obtain significant results. We will use the remaining 10% for the testing dataset (final model evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = None\n",
    "val_idx  = None\n",
    "test_idx = None\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[train_idx]\n",
    "X_val  = X[val_idx]\n",
    "X_test = X[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_val  = y[val_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `find_best_triplet`. Note, this might take a while since there are hundreds of possible feature combinations. This is a good chance to check your gradient descent implementation and make sure it is efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_triplet(X_train, y_train, X_val, y_val, triplets, alpha, num_iter):\n",
    "    \"\"\"\n",
    "    Iterate over all possible triplets and find the triplet that best \n",
    "    minimizes the cost function. You should first preprocess the data \n",
    "    and obtain a array containing the columns corresponding to the\n",
    "    triplet. Don't forget the bias trick.\n",
    "\n",
    "    Input:\n",
    "    - X_train: training dataset.\n",
    "    - y_train: training labels.\n",
    "    - X_val: validation dataset.\n",
    "    - y_val: validation labels.\n",
    "    - triplets: a list of three features in X.\n",
    "    - alpha: The value of the best alpha previously found.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Output:\n",
    "    - The best triplet.\n",
    "    \"\"\"\n",
    "    best_triplet = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return best_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64bf74a89ba3a0b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "find_best_triplet(X_train, y_train, X_val, y_val, triplets, alpha=best_alpha, num_iter=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "As you have seen in class, train the model using one feature at a time, and choose the best single feature (use the validation dataset and save the feature for which you obtain the best loss value). Next, check which feature performs best when added to the feature you previously chose. Repeat this process until you reach 3 features + bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the testing dataset and report your findings. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
